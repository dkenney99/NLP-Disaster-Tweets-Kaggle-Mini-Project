{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NLP with Disaster Tweets: Classifying Tweets as Disaster vs Not Disaster\n\n**Course mini-project**  \n**Competition**: Kaggle \"Natural Language Processing with Disaster Tweets\"  \n**Goal**: Predict whether a tweet refers to a real disaster (binary classification).\n\n## Brief description of the problem and data\nWe are given short social media texts (tweets) with metadata (`keyword`, `location`) and a binary label `target` for the training set. The task is supervised text classification.\n\n- Train rows: will report below from code\n- Test rows: will report below from code\n- Columns: `id`, `keyword` (str, may be missing), `location` (str, may be missing), `text` (str), and `target` (0 or 1, train only)\n\n**NLP context**: This is a standard short-text classification problem. We will compare a classic bag-of-words baseline (TF-IDF + Logistic Regression) and a simple sequential neural network with a learnable embedding and a Bidirectional LSTM. We will perform basic EDA, describe cleaning steps, train models, run a small hyperparameter search, analyze results, and generate a Kaggle submission.\n\n**GitHub repository URL**: Add your repo link here once created.\n\n","metadata":{}},{"cell_type":"markdown","source":"# 1) Setup, imports, and configuration","metadata":{}},{"cell_type":"code","source":"# 1) Setup\nimport os\nimport re\nimport random\nimport html\nimport json\nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, classification_report, confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\nrandom.seed(RANDOM_STATE)\ntf.random.set_seed(RANDOM_STATE)\n\n# File paths (Kaggle default)\nTRAIN_PATH = \"train.csv\"\nTEST_PATH  = \"test.csv\"\n\nassert os.path.exists(TRAIN_PATH) and os.path.exists(TEST_PATH), \"Place train.csv and test.csv in the working directory\"\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-22T21:25:54.076836Z","iopub.execute_input":"2025-10-22T21:25:54.077248Z","iopub.status.idle":"2025-10-22T21:25:59.735676Z","shell.execute_reply.started":"2025-10-22T21:25:54.077219Z","shell.execute_reply":"2025-10-22T21:25:59.734399Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2) Load data and basic structure summary","metadata":{}},{"cell_type":"code","source":"# 2) Load data and basic shape/summary\ntrain_df = pd.read_csv(TRAIN_PATH)\ntest_df  = pd.read_csv(TEST_PATH)\n\ndisplay(train_df.head())\ndisplay(test_df.head())\n\nprint(\"Train shape:\", train_df.shape)\nprint(\"Test shape:\", test_df.shape)\nprint(\"\\nTrain columns:\", train_df.columns.tolist())\nprint(\"Test columns:\", test_df.columns.tolist())\n\nprint(\"\\nNull counts (train):\")\nprint(train_df.isna().sum())\n\nprint(\"\\nClass balance (train target):\")\nprint(train_df[\"target\"].value_counts(normalize=True).rename(\"proportion\").round(3))\n\n# Brief text length stats\ntrain_text_len = train_df[\"text\"].astype(str).str.split().map(len)\nprint(\"\\nToken length stats (train text):\")\nprint(train_text_len.describe())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3) EDA visuals and quick inspection","metadata":{}},{"cell_type":"code","source":"# 3) EDA: basic visualizations with matplotlib only\n\nfig, ax = plt.subplots()\ntrain_df[\"target\"].value_counts().sort_index().plot(kind=\"bar\", ax=ax)\nax.set_title(\"Target distribution\")\nax.set_xlabel(\"target\")\nax.set_ylabel(\"count\")\nplt.show()\n\nfig, ax = plt.subplots()\ntrain_text_len.plot(kind=\"hist\", bins=30, ax=ax)\nax.set_title(\"Distribution of tweet token lengths\")\nax.set_xlabel(\"tokens\")\nplt.show()\n\n# Top keywords (non-null)\ntop_kw = train_df[\"keyword\"].dropna().str.lower().value_counts().head(20)\nfig, ax = plt.subplots()\ntop_kw.plot(kind=\"bar\", ax=ax)\nax.set_title(\"Top 20 keywords\")\nax.set_ylabel(\"count\")\nplt.xticks(rotation=75)\nplt.show()\n\n# Missingness bars\nfig, ax = plt.subplots()\ntrain_df[[\"keyword\", \"location\", \"text\"]].isna().sum().plot(kind=\"bar\", ax=ax)\nax.set_title(\"Missing values per column (train)\")\nax.set_ylabel(\"count missing\")\nplt.show()\n\nprint(\"Plan of analysis:\")\nprint(\"- Clean text minimally, keep helpful tokens like hashtags and user/url markers.\")\nprint(\"- Build two models: TF-IDF + Logistic Regression baseline, and a BiLSTM neural model with a learnable embedding.\")\nprint(\"- Run a small hyperparameter search on the neural model.\")\nprint(\"- Compare validation F1 and produce a Kaggle submission from the better model.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4) Light text cleaning utilities","metadata":{}},{"cell_type":"code","source":"# 4) Cleaning helpers\n\nURL_RE    = re.compile(r\"https?://\\S+|www\\.\\S+\")\nUSER_RE   = re.compile(r\"@\\w+\")\nHASH_RE   = re.compile(r\"#(\\w+)\")\nNUM_RE    = re.compile(r\"\\b\\d+\\b\")\nWS_RE     = re.compile(r\"\\s+\")\n\ndef clean_text(s: str) -> str:\n    if not isinstance(s, str):\n        return \"\"\n    s = html.unescape(s)\n    s = URL_RE.sub(\" URL \", s)\n    s = USER_RE.sub(\" USER \", s)\n    s = HASH_RE.sub(r\" \\1 HASHTAG \", s)\n    s = NUM_RE.sub(\" NUM \", s)\n    s = s.replace(\"&amp;\", \" and \")\n    s = WS_RE.sub(\" \", s).strip()\n    return s.lower()\n\ndef join_fields(df: pd.DataFrame) -> pd.Series:\n    # tag each field so models can learn source\n    kw  = df[\"keyword\"].fillna(\"\").astype(str).str.lower().str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n    loc = df[\"location\"].fillna(\"\").astype(str).str.lower().str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n    txt = df[\"text\"].astype(str).map(clean_text)\n    return \"kw: \" + kw + \" loc: \" + loc + \" txt: \" + txt\n\ntrain_joined = join_fields(train_df)\ntest_joined  = join_fields(test_df)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5) Baseline model: TF-IDF + Logistic Regression","metadata":{}},{"cell_type":"code","source":"# 5) Baseline: TF-IDF + Logistic Regression\n\nX_train_base, X_val_base, y_train_base, y_val_base = train_test_split(\n    train_joined, train_df[\"target\"].astype(int), \n    test_size=0.2, stratify=train_df[\"target\"].astype(int), random_state=RANDOM_STATE\n)\n\ntfidf = TfidfVectorizer(ngram_range=(1,2), min_df=2, max_df=0.98, strip_accents=\"unicode\", sublinear_tf=True)\nXtr = tfidf.fit_transform(X_train_base)\nXva = tfidf.transform(X_val_base)\n\nlr = LogisticRegression(solver=\"liblinear\", C=2.0, class_weight=\"balanced\", max_iter=200, random_state=RANDOM_STATE)\nlr.fit(Xtr, y_train_base)\nval_pred_lr = lr.predict(Xva)\nval_f1_lr = f1_score(y_val_base, val_pred_lr)\nprint(f\"Baseline TF-IDF + LogisticRegression validation F1: {val_f1_lr:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6) Tokenization for the neural model","metadata":{}},{"cell_type":"code","source":"# 6) Tokenize text for neural model\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nMAX_VOCAB = 20000\nOOV_TOKEN = \"<oov>\"\nMAX_LEN   = 40   # tweets are short; adjust if needed\n\ntokenizer = Tokenizer(num_words=MAX_VOCAB, oov_token=OOV_TOKEN)\ntokenizer.fit_on_texts(list(train_joined))\n\nX_seq = tokenizer.texts_to_sequences(list(train_joined))\nX_pad = pad_sequences(X_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\ny     = train_df[\"target\"].astype(int).values\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_pad, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n)\n\nprint(\"Tokenized shapes:\", X_train.shape, X_val.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7) Define a simple sequential neural architecture (Embedding + BiLSTM)","metadata":{}},{"cell_type":"code","source":"# 7) Model builder: Embedding + Bidirectional LSTM\ndef build_bilstm_model(\n    vocab_size: int,\n    max_len: int,\n    embedding_dim: int = 128,\n    lstm_units: int = 64,\n    dropout_rate: float = 0.2,\n    lr: float = 1e-3\n) -> keras.Model:\n    inputs = keras.Input(shape=(max_len,), dtype=\"int32\")\n    x = layers.Embedding(vocab_size, embedding_dim, input_length=max_len)(inputs)\n    x = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=False))(x)\n    x = layers.Dropout(dropout_rate)(x)\n    x = layers.Dense(64, activation=\"relu\")(x)\n    x = layers.Dropout(dropout_rate)(x)\n    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n    model = keras.Model(inputs, outputs)\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=lr),\n        loss=\"binary_crossentropy\",\n        metrics=[keras.metrics.AUC(name=\"auc\")]\n    )\n    return model\n\nVOCAB_SIZE = min(MAX_VOCAB, len(tokenizer.word_index) + 1)\nmodel = build_bilstm_model(VOCAB_SIZE, MAX_LEN)\nmodel.summary()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 8) Train with early stopping, then evaluate with validation F1","metadata":{}},{"cell_type":"code","source":"# 8) Train and evaluate\ncallbacks = [\n    keras.callbacks.EarlyStopping(monitor=\"val_auc\", patience=3, mode=\"max\", restore_best_weights=True)\n]\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=10,\n    batch_size=64,\n    callbacks=callbacks,\n    verbose=1\n)\n\n# Evaluate with F1 at threshold 0.5\nval_pred_prob = model.predict(X_val).ravel()\nval_pred      = (val_pred_prob >= 0.5).astype(int)\nval_f1_nn     = f1_score(y_val, val_pred)\n\nprint(f\"Neural model validation F1: {val_f1_nn:.4f}\")\nprint(\"\\nClassification report:\")\nprint(classification_report(y_val, val_pred, digits=4))\n\n# Confusion matrix plot\ncm = confusion_matrix(y_val, val_pred)\nfig, ax = plt.subplots()\nim = ax.imshow(cm, interpolation=\"nearest\")\nax.set_title(\"Confusion matrix (validation)\")\nplt.colorbar(im)\nax.set_xticks([0,1]); ax.set_yticks([0,1])\nax.set_xticklabels([\"pred 0\",\"pred 1\"]); ax.set_yticklabels([\"true 0\",\"true 1\"])\nfor i in range(cm.shape[0]):\n    for j in range(cm.shape[1]):\n        ax.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\nplt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 9) Small hyperparameter search over the neural model","metadata":{}},{"cell_type":"code","source":"# 9) Manual hyperparameter search (small and fast)\nsearch_space = [\n    {\"embedding_dim\": 64,  \"lstm_units\": 64,  \"dropout_rate\": 0.2, \"lr\": 1e-3, \"batch_size\": 64},\n    {\"embedding_dim\": 128, \"lstm_units\": 64,  \"dropout_rate\": 0.3, \"lr\": 1e-3, \"batch_size\": 64},\n    {\"embedding_dim\": 128, \"lstm_units\": 96,  \"dropout_rate\": 0.3, \"lr\": 1e-3, \"batch_size\": 64},\n    {\"embedding_dim\": 128, \"lstm_units\": 64,  \"dropout_rate\": 0.5, \"lr\": 8e-4, \"batch_size\": 64},\n]\n\nresults = []\nfor i, hp in enumerate(search_space, 1):\n    print(f\"\\nTrial {i}/{len(search_space)}: {hp}\")\n    tf.keras.backend.clear_session()\n    m = build_bilstm_model(\n        VOCAB_SIZE, MAX_LEN,\n        embedding_dim=hp[\"embedding_dim\"],\n        lstm_units=hp[\"lstm_units\"],\n        dropout_rate=hp[\"dropout_rate\"],\n        lr=hp[\"lr\"]\n    )\n    cb = [keras.callbacks.EarlyStopping(monitor=\"val_auc\", patience=2, mode=\"max\", restore_best_weights=True)]\n    hist = m.fit(\n        X_train, y_train,\n        validation_data=(X_val, y_val),\n        epochs=8,\n        batch_size=hp[\"batch_size\"],\n        callbacks=cb,\n        verbose=0\n    )\n    # F1 on validation\n    probs = m.predict(X_val, verbose=0).ravel()\n    preds = (probs >= 0.5).astype(int)\n    f1 = f1_score(y_val, preds)\n    best_auc = max(hist.history[\"val_auc\"])\n    results.append({\"params\": hp, \"val_f1\": f1, \"best_val_auc\": best_auc})\n\nresults_df = pd.DataFrame(results).sort_values(by=\"val_f1\", ascending=False).reset_index(drop=True)\ndisplay(results_df)\n\n# Plot F1 scores\nfig, ax = plt.subplots()\nax.plot(range(1, len(results_df)+1), results_df[\"val_f1\"], marker=\"o\")\nax.set_xticks(range(1, len(results_df)+1))\nax.set_xlabel(\"trial rank (best to worst)\")\nax.set_ylabel(\"validation F1\")\nax.set_title(\"Hyperparameter search results\")\nplt.show()\n\nbest_hp = results_df.loc[0, \"params\"]\nprint(\"Best hyperparameters:\", best_hp)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 10) Retrain best neural model on all training data and generate submission","metadata":{}},{"cell_type":"code","source":"# 10) Retrain best model on full data and create submission\n\n# Re-split tokenizer texts for the full training set\nX_all = X_pad\ny_all = y\n\ntf.keras.backend.clear_session()\nbest_model = build_bilstm_model(\n    VOCAB_SIZE, MAX_LEN,\n    embedding_dim=best_hp[\"embedding_dim\"],\n    lstm_units=best_hp[\"lstm_units\"],\n    dropout_rate=best_hp[\"dropout_rate\"],\n    lr=best_hp[\"lr\"]\n)\n\ncallbacks = [\n    keras.callbacks.EarlyStopping(monitor=\"auc\", patience=1, mode=\"max\", restore_best_weights=True)\n]\n\nbest_model.fit(\n    X_all, y_all,\n    epochs=6,\n    batch_size=best_hp[\"batch_size\"],\n    validation_split=0.05,\n    callbacks=callbacks,\n    verbose=1\n)\n\n# Prepare test features\ntest_seq = tokenizer.texts_to_sequences(list(test_joined))\ntest_pad = pad_sequences(test_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n\n# Predict and build submission\ntest_prob = best_model.predict(test_pad).ravel()\ntest_pred = (test_prob >= 0.5).astype(int)\n\nsubmission = pd.DataFrame({\"id\": test_df[\"id\"], \"target\": test_pred})\nsubmission_path = \"submission.csv\"\nsubmission.to_csv(submission_path, index=False)\nprint(\"Saved submission to:\", submission_path)\ndisplay(submission.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Results and Analysis\n\nWe report the main validation result using an 80/20 stratified split.\n\n- TF-IDF + Logistic Regression: see `val_f1_lr` value from the baseline cell\n- Neural model (Embedding + BiLSTM), best trial: see `results_df` table for `val_f1` and `best_val_auc`\n- We used early stopping on AUC to control overfitting.\n- We tested a small grid across embedding dimensions, LSTM units, dropout, and learning rate.\n\nObservations to include in your own words:\n- Which model performed better and why that might be the case for this dataset of short texts\n- What cleaning choices affected performance\n- What hyperparameters helped and any signs of overfitting or underfitting\n- Limitations and simple next steps\n","metadata":{}}]}